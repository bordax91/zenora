import { NextResponse } from 'next/server'

export async function POST(req) {
  const { message } = await req.json()

  try {
    const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.GROQ_API_KEY}`
      },
      body: JSON.stringify({
        model: 'llama3-70b-8192',
        messages: [
          { role: 'system', content: "Tu es Zenora, une IA de soutien Ã©motionnel basÃ© sur la santÃ© mentale. Imagine Ãªtre psychologue et suivre une personne souffrant d'un problÃ¨me liÃ© Ã  la santÃ© mentale(anxiÃ©tÃ©, stress, solitude, besoin d'Ã©coute, ...) et ayant besoin de soutiens humains, Ã©motionnels et pratiques en utilisant le tutoiement. Tu simules une conversation avec l'utilisateur afin de trouver une solution Ã  ses problÃ¨mes.Il ne faut pas faire des rÃ©ponses trop longue. Tu continues la conversation tant que l'utilisateur ne te demande pas d'arrÃªter." },
          { role: 'user', content: message }
        ],
        temperature: 0.7
      })
    })

    const data = await response.json()

    // VÃ©rification de l'erreur
    if (!response.ok) {
      console.error('Erreur Groq:', data)
      return NextResponse.json({ response: "DÃ©solÃ©, je rencontre un problÃ¨me technique. ğŸ˜”" }, { status: 500 })
    }

    const botMessage = data.choices?.[0]?.message?.content || "Je suis lÃ  pour toi. ğŸŒ¿"

    return NextResponse.json({ response: botMessage })

  } catch (error) {
    console.error('Erreur serveur:', error)
    return NextResponse.json({ response: "Oups, une erreur s'est produite. ğŸ˜”" }, { status: 500 })
  }
}
